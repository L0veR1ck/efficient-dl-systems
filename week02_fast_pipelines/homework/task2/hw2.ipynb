{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882ed53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget \"https://www.dropbox.com/scl/fi/e6oqpx6iuos7kn9m139z7/wikitext-103-raw-v1.zip?rlkey=81evwbaqfkxtckj8zhks7yied&st=6ept2pdm&dl=0\"\n",
    "# !unzip -q \"wikitext-103-raw-v1.zip?rlkey=81evwbaqfkxtckj8zhks7yied&st=6ept2pdm&dl=0\"\n",
    "# !rm -rf \"wikitext-103-raw-v1.zip?rlkey=81evwbaqfkxtckj8zhks7yied&st=6ept2pdm&dl=0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "217feeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "\n",
    "from collections import defaultdict\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import Sampler, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformer import TransformerModel, generate_square_subsequent_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e62512",
   "metadata": {},
   "source": [
    "For each of the implemented methods (and all variations of the third and fourth methods), mock one training epoch and measure minimum, maximum, mean and median batch processing times.\n",
    "To mock a training epoch, you need to construct a small GPT-2-like model: use `nn.Embedding` layer, `PositionalEncoding` class from `transformer.py` file and a single `nn.TransformerDecoder` layer with a hidden size of 1024 and 8 heads.\n",
    "For tokenization, use the `.tokenize()` method of `AutoTokenizer.from_pretrained(\"bert-base-uncased\")`.\n",
    "Run one epoch **without a backward pass** to measure the iteration time.\n",
    "Make sure you've [warmed up](https://forums.developer.nvidia.com/t/why-warm-up/48565) the GPU before computing the statistics and do not forget about asynchronous CUDA kernel execution.\n",
    "\n",
    "Keep in mind that all padding in this task must be **implemented by you**: unlike the seminar, PyTorch’s default collation padding is not allowed.\n",
    "For all subproblems, drop all sequences exceeding 640 tokens.\n",
    "Feel free to modify the keyword arguments of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbdd341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt2_model() -> torch.nn.Module:\n",
    "    return TransformerModel(\n",
    "        ntoken=30523,\n",
    "        d_model=256,\n",
    "        nhead=8,\n",
    "        d_hid=1024,\n",
    "        nlayers=1,\n",
    "        dropout=0.3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bc30e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(dataloader):\n",
    "    model = get_gpt2_model()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else 1/0\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= 500:\n",
    "                break\n",
    "\n",
    "            batch = batch.to(device)\n",
    "            mask = generate_square_subsequent_mask(batch.shape[0]).to(device)\n",
    "            model(batch, mask)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = batch.to(device)\n",
    "            mask = generate_square_subsequent_mask(batch.shape[0]).to(device)\n",
    "\n",
    "            start = time.perf_counter()\n",
    "            model(batch, mask)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.perf_counter()\n",
    "\n",
    "            times.append(end - start)\n",
    "\n",
    "    times = torch.tensor(times)\n",
    "    stats = {\n",
    "        \"min\": float(times.min()),\n",
    "        \"max\": float(times.max()),\n",
    "        \"mean\": float(times.mean()),\n",
    "        \"median\": float(times.median()),\n",
    "    }\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ce74e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"wikitext-103-raw-v1\"\n",
    "MAX_LENGTH = 640\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dc8ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path: str):\n",
    "    with open(data_path + \"/train-00000-of-00002.txt\", \"r\") as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    with open(data_path + \"/train-00001-of-00002.txt\", \"r\") as f:\n",
    "        data += f.readlines()\n",
    "\n",
    "    data = [el.strip(\" \\n=\") for el in data]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1066c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        data_path: str, \n",
    "        max_length: int = MAX_LENGTH,\n",
    "        tokenizer: Any = tokenizer,\n",
    "    ):\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.data = read_data(data_path)\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> str:\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe2c0f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitext_dataset = WikiTextDataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf56584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brain_collate_fn(\n",
    "    batch: list[str], \n",
    "    max_length: Optional[int] = MAX_LENGTH,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    text_list = []\n",
    "    for _text in batch:\n",
    "        processed_text = tokenizer(\n",
    "            _text, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "        pad_size = max_length - len(processed_text)\n",
    "        processed_text = torch.cat(\n",
    "            [\n",
    "                processed_text, \n",
    "                torch.full((pad_size, ), tokenizer.pad_token_id)\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        text_list.append(processed_text)\n",
    "    \n",
    "    text_list = torch.stack(text_list, dim=0)\n",
    "\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b8af5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18204/18204 [19:00<00:00, 15.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'min': 0.027524013072252274,\n",
       " 'max': 0.08858683705329895,\n",
       " 'mean': 0.0510968379676342,\n",
       " 'median': 0.049413640052080154}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain_dataloader = DataLoader(\n",
    "    wikitext_dataset,\n",
    "    collate_fn=brain_collate_fn,\n",
    "    batch_size=64,\n",
    ")\n",
    "run_epoch(brain_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afdd3d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_brain_collate_fn(\n",
    "    batch: list[str], \n",
    "    max_length: Optional[int] = MAX_LENGTH,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    processed_text_list = [\n",
    "        tokenizer(\n",
    "            _text, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "        for _text in batch\n",
    "    ]\n",
    "    max_length = max(len(_text) for _text in processed_text_list)\n",
    "    text_list = []\n",
    "    for processed_text in processed_text_list:\n",
    "        pad_size = max_length - len(processed_text)\n",
    "        processed_text = torch.cat(\n",
    "            [\n",
    "                processed_text, \n",
    "                torch.full((pad_size, ), tokenizer.pad_token_id)\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        text_list.append(processed_text)\n",
    "    \n",
    "    text_list = torch.stack(text_list, dim=0)\n",
    "\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fe22e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18204/18204 [12:00<00:00, 25.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'min': 0.0015366340521723032,\n",
       " 'max': 0.060972291976213455,\n",
       " 'mean': 0.02851749025285244,\n",
       " 'median': 0.027600599452853203}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_brain_dataloader = DataLoader(\n",
    "    wikitext_dataset,\n",
    "    collate_fn=big_brain_collate_fn,\n",
    "    batch_size=64,\n",
    ")\n",
    "run_epoch(big_brain_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5fb493d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiTextDatasetTokenized(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        data_path: str, \n",
    "        max_length: int = MAX_LENGTH,\n",
    "        tokenizer: Any = tokenizer,\n",
    "    ):\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.data = [\n",
    "            self.tokenizer(\n",
    "                sentence, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=self.max_length,\n",
    "            )[\"input_ids\"].squeeze(0)\n",
    "            for sentence in read_data(data_path)\n",
    "        ]\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> str:\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0abbb133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UltraBigBrainBatchSampler(torch.utils.data.BatchSampler):\n",
    "    def __init__(self, dataset, batch_size, k):\n",
    "        self.batch_size = batch_size\n",
    "        self.k = k\n",
    "\n",
    "        self.length_to_indices = defaultdict(list)\n",
    "        for i, item in enumerate(dataset):\n",
    "            self.length_to_indices[len(item)].append(i)\n",
    "\n",
    "        self.lengths = list(self.length_to_indices.keys())\n",
    "        self.num_batches = len(dataset) // batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.num_batches):\n",
    "            min_len = random.choice(self.lengths)\n",
    "            \n",
    "            candidates = []\n",
    "            for l in range(min_len, min_len + self.k + 1):\n",
    "                candidates += self.length_to_indices[l]\n",
    "\n",
    "            if not candidates:\n",
    "                continue\n",
    "\n",
    "            batch = random.sample(\n",
    "                candidates,\n",
    "                k=min(self.batch_size, len(candidates))\n",
    "            )\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "81ba8e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ultra_big_brain_collate_fn(\n",
    "    batch: list[str], \n",
    "    max_length: Optional[int] = MAX_LENGTH,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    max_length = max(len(_text) for _text in batch)\n",
    "    text_list = []\n",
    "    for processed_text in batch:\n",
    "        pad_size = max_length - len(processed_text)\n",
    "        processed_text = torch.cat(\n",
    "            [\n",
    "                processed_text, \n",
    "                torch.full((pad_size, ), tokenizer.pad_token_id)\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        text_list.append(processed_text)\n",
    "    \n",
    "    text_list = torch.stack(text_list, dim=0)\n",
    "\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a2e88ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = WikiTextDatasetTokenized(\n",
    "    data_path,\n",
    "    MAX_LENGTH,\n",
    "    tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8a24fea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ultra_big_brain_batch_sampler = UltraBigBrainBatchSampler(\n",
    "    dataset=tokenized_dataset,\n",
    "    batch_size=64,\n",
    "    k=5,\n",
    ")\n",
    "ultra_big_brain_dataloader = torch.utils.data.DataLoader(\n",
    "    tokenized_dataset,\n",
    "    collate_fn=ultra_big_brain_collate_fn,\n",
    "    batch_sampler=ultra_big_brain_batch_sampler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dfe59c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 5, 3, 0, 4, 2, 4, 3, 1, 5, 1, 1, 2, 1, 4, 4, 4, 4, 1, 0, 2, 2, 1,\n",
       "        4, 1, 1, 3, 1, 1, 3, 3, 5, 4, 1, 0, 0, 0, 4, 3, 1, 1, 3, 4, 3, 4, 0, 4,\n",
       "        1, 5, 5, 4, 0, 2, 2, 3, 4, 0, 1, 2, 4, 2, 4, 0])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(next(iter(ultra_big_brain_dataloader))==0).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6acbd7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18203/18203 [07:39<00:00, 39.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'min': 0.0008770970162004232,\n",
       " 'max': 0.0751737430691719,\n",
       " 'mean': 0.02448800764977932,\n",
       " 'median': 0.024018418043851852}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_epoch(ultra_big_brain_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ysda (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
